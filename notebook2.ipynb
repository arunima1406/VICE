{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d82138d6399c8d1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T13:06:12.986801Z",
     "start_time": "2025-05-17T13:06:12.977657Z"
    }
   },
   "id": "9d46ae0abee97af7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Define Custom Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e61ac7d1e346fccb"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CarDamageDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(label_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0]  \n",
    "        label = self.labels.iloc[idx, 1]     \n",
    "\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label), img_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T13:06:17.943628Z",
     "start_time": "2025-05-17T13:06:17.931703Z"
    }
   },
   "id": "2cc032d892edb6f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Prepare DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec50b64a991f7b48"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CarDamageDataset(\n",
    "    image_dir='data/train/images',\n",
    "    label_csv='data/train/train.csv',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T13:06:24.427410Z",
     "start_time": "2025-05-17T13:06:24.399112Z"
    }
   },
   "id": "767cc7cc7f628704"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Load Pretrained ViT Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc346fca64b534f7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m ViTForImageClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgoogle/vit-base-patch16-224\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m     num_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m  \u001B[38;5;66;03m# Update this to match number of classes\u001B[39;00m\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      9\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001B[0m, in \u001B[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4399\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   4389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4390\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   4392\u001B[0m     (\n\u001B[1;32m   4393\u001B[0m         model,\n\u001B[1;32m   4394\u001B[0m         missing_keys,\n\u001B[1;32m   4395\u001B[0m         unexpected_keys,\n\u001B[1;32m   4396\u001B[0m         mismatched_keys,\n\u001B[1;32m   4397\u001B[0m         offload_index,\n\u001B[1;32m   4398\u001B[0m         error_msgs,\n\u001B[0;32m-> 4399\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_load_pretrained_model(\n\u001B[1;32m   4400\u001B[0m         model,\n\u001B[1;32m   4401\u001B[0m         state_dict,\n\u001B[1;32m   4402\u001B[0m         checkpoint_files,\n\u001B[1;32m   4403\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m   4404\u001B[0m         ignore_mismatched_sizes\u001B[38;5;241m=\u001B[39mignore_mismatched_sizes,\n\u001B[1;32m   4405\u001B[0m         sharded_metadata\u001B[38;5;241m=\u001B[39msharded_metadata,\n\u001B[1;32m   4406\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[1;32m   4407\u001B[0m         disk_offload_folder\u001B[38;5;241m=\u001B[39moffload_folder,\n\u001B[1;32m   4408\u001B[0m         offload_state_dict\u001B[38;5;241m=\u001B[39moffload_state_dict,\n\u001B[1;32m   4409\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mtorch_dtype,\n\u001B[1;32m   4410\u001B[0m         hf_quantizer\u001B[38;5;241m=\u001B[39mhf_quantizer,\n\u001B[1;32m   4411\u001B[0m         keep_in_fp32_regex\u001B[38;5;241m=\u001B[39mkeep_in_fp32_regex,\n\u001B[1;32m   4412\u001B[0m         device_mesh\u001B[38;5;241m=\u001B[39mdevice_mesh,\n\u001B[1;32m   4413\u001B[0m         key_mapping\u001B[38;5;241m=\u001B[39mkey_mapping,\n\u001B[1;32m   4414\u001B[0m         weights_only\u001B[38;5;241m=\u001B[39mweights_only,\n\u001B[1;32m   4415\u001B[0m     )\n\u001B[1;32m   4417\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   4418\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4833\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[1;32m   4831\u001B[0m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[1;32m   4832\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[0;32m-> 4833\u001B[0m     disk_offload_index, cpu_offload_index \u001B[38;5;241m=\u001B[39m _load_state_dict_into_meta_model(\n\u001B[1;32m   4834\u001B[0m         model_to_load,\n\u001B[1;32m   4835\u001B[0m         state_dict,\n\u001B[1;32m   4836\u001B[0m         shard_file,\n\u001B[1;32m   4837\u001B[0m         expected_keys,\n\u001B[1;32m   4838\u001B[0m         reverse_key_renaming_mapping,\n\u001B[1;32m   4839\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[1;32m   4840\u001B[0m         disk_offload_folder\u001B[38;5;241m=\u001B[39mdisk_offload_folder,\n\u001B[1;32m   4841\u001B[0m         disk_offload_index\u001B[38;5;241m=\u001B[39mdisk_offload_index,\n\u001B[1;32m   4842\u001B[0m         cpu_offload_folder\u001B[38;5;241m=\u001B[39mcpu_offload_folder,\n\u001B[1;32m   4843\u001B[0m         cpu_offload_index\u001B[38;5;241m=\u001B[39mcpu_offload_index,\n\u001B[1;32m   4844\u001B[0m         hf_quantizer\u001B[38;5;241m=\u001B[39mhf_quantizer,\n\u001B[1;32m   4845\u001B[0m         is_safetensors\u001B[38;5;241m=\u001B[39mis_offloaded_safetensors,\n\u001B[1;32m   4846\u001B[0m         keep_in_fp32_regex\u001B[38;5;241m=\u001B[39mkeep_in_fp32_regex,\n\u001B[1;32m   4847\u001B[0m         unexpected_keys\u001B[38;5;241m=\u001B[39munexpected_keys,\n\u001B[1;32m   4848\u001B[0m         device_mesh\u001B[38;5;241m=\u001B[39mdevice_mesh,\n\u001B[1;32m   4849\u001B[0m     )\n\u001B[1;32m   4851\u001B[0m \u001B[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001B[39;00m\n\u001B[1;32m   4852\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m state_dict\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:824\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled():\n\u001B[1;32m    822\u001B[0m         param_device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 824\u001B[0m     _load_parameter_into_model(model, param_name, param\u001B[38;5;241m.\u001B[39mto(param_device))\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(\n\u001B[1;32m    828\u001B[0m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001B[1;32m    829\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:712\u001B[0m, in \u001B[0;36m_load_parameter_into_model\u001B[0;34m(model, param_name, tensor)\u001B[0m\n\u001B[1;32m    710\u001B[0m module, param_type \u001B[38;5;241m=\u001B[39m get_module_from_name(model, param_name)\n\u001B[1;32m    711\u001B[0m \u001B[38;5;66;03m# This will check potential shape mismatch if skipped before\u001B[39;00m\n\u001B[0;32m--> 712\u001B[0m module\u001B[38;5;241m.\u001B[39mload_state_dict({param_type: tensor}, strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, assign\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2585\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2586\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2587\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2588\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2589\u001B[0m             ),\n\u001B[1;32m   2590\u001B[0m         )\n\u001B[1;32m   2592\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2593\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2594\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2595\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2596\u001B[0m         )\n\u001B[1;32m   2597\u001B[0m     )\n\u001B[1;32m   2598\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=3  # Update this to match number of classes\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-17T13:06:29.832909Z",
     "start_time": "2025-05-17T13:06:28.177468Z"
    }
   },
   "id": "73a97ea73d5d42e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    " 5. Training Loop with Error Handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b0f0b638ceccd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_number(filename):\n",
    "    matches = re.findall(r'\\d+', filename)\n",
    "    return int(matches[0]) if matches else None\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels, filenames in tqdm(train_loader):\n",
    "        try:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"🔥 Error: {e}\")\n",
    "            for fname in filenames:\n",
    "                try:\n",
    "                    num = extract_number(fname)\n",
    "                    print(f\"✅ Extracted number: {num} from {fname}\")\n",
    "                except:\n",
    "                    print(f\"❌ Could not extract number from: {fname}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"📉 Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd56c59c3cef54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b12924ed9c918bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 🧪 Model Evaluation on Test Set\n",
    "\n",
    "test_dataset = CarDamageDataset(\n",
    "    image_dir='data/test/images',\n",
    "    label_csv='data/test/test.csv',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"✅ Test Accuracy: {accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b93413a5bd1f35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Save the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97fee2898e58193a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_PATH = \"car_dent_transformer.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"✅ Model saved to {MODEL_PATH}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8458542a7082e96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
